{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d894191d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 145 ms (started: 2022-04-29 06:56:08 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# import all needed libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pymssql\n",
    "from datetime import datetime\n",
    "\n",
    "import copy\n",
    "%load_ext autotime\n",
    "from datetime import datetime as dt\n",
    "import googletrans    \n",
    "from googletrans import Translator\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21daaaf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab7334f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 18.7 s (started: 2022-04-29 06:56:08 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# distilbert-base-uncased-finetuned-sst-2-english\n",
    "# Trained on BERT base model as a teacher\n",
    "from transformers import pipeline\n",
    "classifier1 = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def bert_distilbert(text):\n",
    "    text = str(text)\n",
    "    if text == 'nan ' or text == '':\n",
    "        return math.nan\n",
    "    return classifier1(text)\n",
    "# This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews\n",
    "# in six languages: English, Dutch, German, French, Spanish and Italian\n",
    "# It predicts the sentiment of the review as a number of stars (between 1 and 5)\n",
    "classifier2 = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')  \n",
    "\n",
    "def bert_multilingual(text):\n",
    "    text = str(text)\n",
    "    if text == 'nan ' or text == '':\n",
    "        return math.nan\n",
    "    return classifier2(text)\n",
    "# The model was fine-tuned and evaluated on 15 data sets from diverse text sources to enhance \n",
    "# generalization across different types of texts (reviews, tweets, etc.)\n",
    "classifier3 = pipeline('sentiment-analysis', model='siebert/sentiment-roberta-large-english')   \n",
    "\n",
    "def roberta_english(text):\n",
    "    text = str(text)\n",
    "    if text == 'nan ' or text == '':\n",
    "        return math.nan\n",
    "    return classifier3(text)\n",
    "# This is a roBERTa-base model trained on ~58M tweets and finetuned for sentiment analysis \n",
    "# with the TweetEval benchmark.\n",
    "# Labels: 0 -> Negative; 1 -> Neutral; 2 -> Positive\n",
    "classifier4 = pipeline('sentiment-analysis', model='cardiffnlp/twitter-roberta-base-sentiment')     \n",
    "\n",
    "def roberta_tweet(text):\n",
    "    text = str(text)\n",
    "    if text == 'nan ' or text == '':\n",
    "        return math.nan\n",
    "    return classifier4(text)\n",
    "\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sid_obj= SentimentIntensityAnalyzer()\n",
    "from sentifish import Sentiment\n",
    "from afinn import Afinn\n",
    "afn = Afinn()\n",
    "\n",
    "def textblob(text):\n",
    "    text = str(text)\n",
    "    res = TextBlob(text)\n",
    "    return res.sentiment.polarity\n",
    "\n",
    "def vader(text):\n",
    "    text = str(text)\n",
    "    temp = sid_obj.polarity_scores(sentence)\n",
    "    return temp['compound']\n",
    "\n",
    "def sentifish(text):\n",
    "    text = str(text)\n",
    "    obj=Sentiment(text)\n",
    "    polarity = obj.analyze( )\n",
    "    return polarity\n",
    "\n",
    "def afinn(text):\n",
    "    text = str(text)\n",
    "    return afn.score(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a668d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 819 ms (started: 2022-04-29 06:56:27 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy import displacy\n",
    "    \n",
    "def dataframe_display(text):\n",
    "    doc = nlp(text)\n",
    "    df = pd.DataFrame(columns=['token','.dep_','.pos_','.tag_','children','children.pos_','children.text'])\n",
    "    for token in doc:\n",
    "        c = []\n",
    "        p = []\n",
    "        t = []\n",
    "        for children in token.children:\n",
    "            c.append(children)\n",
    "            p.append(children.pos_)\n",
    "            t.append(children.text)\n",
    "        df2 = {'token':token,\n",
    "              '.dep_':token.dep_,\n",
    "              '.pos_':token.pos_,\n",
    "              '.tag_':token.tag_,\n",
    "              'children':c,\n",
    "              'children.pos_':p,\n",
    "              'children.text':t}\n",
    "        df = df.append(df2, ignore_index = True)\n",
    "    display(pd_centered(df))\n",
    "    \n",
    "def pos_display(text):\n",
    "    doc = nlp(text)\n",
    "    sentence_spans = list(doc.sents)\n",
    "    options = {'distance': 100}\n",
    "    displacy.render(sentence_spans, style='dep',jupyter=True,options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "770a6e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 622 µs (started: 2022-04-29 06:56:27 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def print_list(lis):\n",
    "    print(\"Length of list:\",len(lis))\n",
    "    for i in range(len(lis)):\n",
    "        print(lis[i])\n",
    "        \n",
    "def pd_centered(df):\n",
    "    return df.style.set_table_styles([\n",
    "        {\"selector\": \"th\", \"props\": [(\"text-align\", \"center\")]},\n",
    "        {\"selector\": \"td\", \"props\": [(\"text-align\", \"center\")]}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c6fc403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 740 ms (started: 2022-04-29 06:56:27 +00:00)\n"
     ]
    }
   ],
   "source": [
    "connection1 = pymssql.connect(server='100.22.1.57',user='aravindh',password='c4o@ER',database ='djuboreview',port='1443')\n",
    "\n",
    "query1 ='select hotelcode,review_content,review_header,sentiment_score,polarity,properties,reviewid_new,id,review_score,language,Websitecode from djuboreview.dbo.reviewdetails_bak with (nolock) where id= 2404466'# and dtcollected ='+dt.today().strftime(\"'%Y-%m-%d'\")\n",
    "\n",
    "df=pd.read_sql(query1,connection1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c2b1b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.73 ms (started: 2022-04-29 06:56:28 +00:00)\n"
     ]
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494af60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17c7445d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.84 ms (started: 2022-04-29 06:56:28 +00:00)\n"
     ]
    }
   ],
   "source": [
    "df['review_content1']=df['review_content']\n",
    "\n",
    "df.loc[df['language']!='en','review_content1']= df[df['language']!='en']['review_content1'].apply(lambda x: translator.translate(x, dest='en').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b9ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd9c47f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c798f27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 26.3 ms (started: 2022-04-29 06:56:28 +00:00)\n"
     ]
    }
   ],
   "source": [
    "df.loc[df['review_content1'].isnull(),'review_content1']='0'\n",
    "\n",
    "df['Sentiment_Score']=0.0\n",
    "\n",
    "df['Sentiment_Score']=df['Sentiment_Score'].astype(float)\n",
    "\n",
    "vb=df[(df['review_content1'].str.contains(\"positive\",case=False)) & (df['review_content1'].str.contains(\"negative\",case=False))|(df['review_content1']=='0')|(df['review_content1']=='There are no comments available for this review')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020fba46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "667fd57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.92 ms (started: 2022-04-29 06:56:28 +00:00)\n"
     ]
    }
   ],
   "source": [
    "vb=vb.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f18eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfcd9a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.24 ms (started: 2022-04-29 06:56:28 +00:00)\n"
     ]
    }
   ],
   "source": [
    "vb.drop('index',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44c38ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a70bb8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b499d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb6403b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.35 ms (started: 2022-04-29 06:56:28 +00:00)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(vb)):\n",
    "    if vb['Websitecode'][i]==2 or vb['Websitecode'][i]==5 or vb['Websitecode'][i]==12:\n",
    "        a = list(np.arange(start=1, stop=10, step=0.8))\n",
    "        OldMax = max(a)\n",
    "        OldMin = min(a)\n",
    "\n",
    "        NewMax = round(1)\n",
    "        NewMin = -1\n",
    "\n",
    "        OldRange = (OldMax - OldMin)  \n",
    "        NewRange = (NewMax - NewMin) \n",
    "\n",
    "        vb['Sentiment_Score'][i] = (((vb['review_score'][i] - OldMin) * NewRange) / OldRange) + NewMin\n",
    "    else:\n",
    "        a = list(np.arange(start=1, stop=5, step=0.8))\n",
    "        OldMax = max(a)\n",
    "        OldMin = min(a)\n",
    "\n",
    "        NewMax = 1\n",
    "        NewMin = -1\n",
    "\n",
    "        OldRange = (OldMax - OldMin)  \n",
    "        NewRange = (NewMax - NewMin) \n",
    "\n",
    "        vb['Sentiment_Score'][i] = (((vb['review_score'][i] - OldMin) * NewRange) / OldRange) + NewMin\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e737f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f1f64b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.17 ms (started: 2022-04-29 06:56:28 +00:00)\n"
     ]
    }
   ],
   "source": [
    "oo=df[~df['id'].isin(vb['id'])]\n",
    "\n",
    "df=pd.concat([oo,vb])\n",
    "\n",
    "df=df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91eebb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ba94301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "time: 11.6 ms (started: 2022-04-29 06:56:28 +00:00)\n"
     ]
    }
   ],
   "source": [
    "cd = pd.DataFrame(columns = ['hotelcode', 'review_header','review_content',\n",
    "         'reviewid_new', 'id','review_score','row','Reviewcontent','ReviewContentPos','ReviewContentNeg','ReviewContentNeu','Sentiment_Score'])\n",
    "\n",
    "counter = 0\n",
    "for i in range(df.shape[0]):\n",
    "#     print(i)\n",
    "    if df.loc[i,'review_content1']=='kkk':\n",
    "        df.loc[i,'review_content1'] = math.nan\n",
    "#         print(i)\n",
    "    try:\n",
    "        if math.isnan(df.loc[i,'review_content1']):\n",
    "            counter += 1\n",
    "            continue\n",
    "    except:\n",
    "        a = 1\n",
    "    \n",
    "    text = df.loc[i,'review_content1']\n",
    "    now = text\n",
    "    \n",
    "    now = now.split('\\n')\n",
    "    pos = math.nan\n",
    "    neg = math.nan\n",
    "    neu = math.nan\n",
    "            \n",
    "    for j in range(len(now)):\n",
    "        x = now[j].split()\n",
    "        if len(x)==0:\n",
    "            continue\n",
    "        if x[0] == '[Positive]:':\n",
    "            pos = ' '.join(x[1:])\n",
    "        elif x[0] == '[Negative]:':\n",
    "            neg = ' '.join(x[1:])\n",
    "        else:\n",
    "            neu = ' '.join(x)\n",
    "    df2 = {'row':i,\n",
    "           'Reviewcontent':df.loc[i,'review_content1'],\n",
    "           'hotelcode':df.loc[i,'hotelcode'],\n",
    "           'review_header':df.loc[i,'review_header'],\n",
    "           'reviewid_new':df.loc[i,'reviewid_new'],\n",
    "           'review_content':df.loc[i,'review_content'],\n",
    "           'Sentiment_Score':df.loc[i,'Sentiment_Score'],\n",
    "           'id':df.loc[i,'id'],\n",
    "           'review_score':df.loc[i,'review_score'],\n",
    "           'ReviewContentPos':pos,\n",
    "           'ReviewContentNeg':neg,\n",
    "           'ReviewContentNeu':neu}\n",
    "    cd = cd.append(df2, ignore_index = True)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade383a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2593b1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.97 ms (started: 2022-04-29 06:56:28 +00:00)\n"
     ]
    }
   ],
   "source": [
    "cd[\"ReviewContentPos\"] = cd[\"ReviewContentPos\"].astype(str)\n",
    "cd[\"ReviewContentNeg\"] = cd[\"ReviewContentNeg\"].astype(str)\n",
    "cd[\"ReviewContentNeu\"] = cd[\"ReviewContentNeu\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dceed17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bed8def5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.6 ms (started: 2022-04-29 06:56:28 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Remove Punctuations without fullstop\n",
    "rem_pun = '!\"#$%&\\'()*+-/:;<=>?@[\\\\]^_`{|}~'\n",
    "def remove_punctuation_without_full(text):\n",
    "    punctuationfree=\"\".join([i if i not in rem_pun else ' ' for i in text])\n",
    "    return punctuationfree\n",
    "\n",
    "# Remove Numbers\n",
    "def remove_numbers(text):\n",
    "    pattern = r'[0-9]'\n",
    "    line = re.sub(pattern,' ', text)\n",
    "    return line\n",
    "\n",
    "# Remove non ascii characters\n",
    "def remove_non_ascii(text):\n",
    "    now = text\n",
    "    now = now.strip().split()\n",
    "    now = [''.join([i for i in word if ord(i) < 128]) for word in now]\n",
    "    text = \"\"\n",
    "    for i in range(len(now)):\n",
    "        text = text + now[i] + \" \"\n",
    "    return text\n",
    "\n",
    "# Convert multiple spaces to single space\n",
    "def convert_multiple_spaces(text):\n",
    "    answer = re.sub(' +', ' ', text)\n",
    "    return answer\n",
    "\n",
    "# Remove first and  end spaces\n",
    "def remove_first_end_spaces(string):\n",
    "    return \"\".join(string.rstrip().lstrip())\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemma(text):\n",
    "    now = text\n",
    "    now = now.strip().split()\n",
    "    now = [lemmatizer.lemmatize(word) for word in now]\n",
    "    text = \"\"\n",
    "    for i in range(len(now)):\n",
    "        text = text + now[i] + \" \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eafc59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Started...\n",
      "Preprocessing Done!\n",
      "time: 1.59 s (started: 2022-04-29 06:56:28 +00:00)\n"
     ]
    }
   ],
   "source": [
    "print('Preprocessing Started...')\n",
    "\n",
    "cd['clean_Reviewcontent'] = cd['Reviewcontent'].apply(lambda x:remove_punctuation_without_full(x))\n",
    "cd['clean_Reviewcontent'] = cd['clean_Reviewcontent'].apply(lambda x:remove_numbers(x))\n",
    "cd['clean_Reviewcontent'] = cd['clean_Reviewcontent'].apply(lambda x:remove_non_ascii(x))\n",
    "cd['clean_Reviewcontent'] = cd['clean_Reviewcontent'].apply(lambda x:lemma(x))\n",
    "cd['clean_Reviewcontent'] = cd['clean_Reviewcontent'].apply(lambda x:convert_multiple_spaces(x))\n",
    "cd['clean_Reviewcontent'] = cd['clean_Reviewcontent'].apply(lambda x:remove_first_end_spaces(x))\n",
    "\n",
    "cd['clean_ReviewContentPos'] = cd['ReviewContentPos'].apply(lambda x:remove_punctuation_without_full(x))\n",
    "cd['clean_ReviewContentPos'] = cd['clean_ReviewContentPos'].apply(lambda x:remove_numbers(x))\n",
    "cd['clean_ReviewContentPos'] = cd['clean_ReviewContentPos'].apply(lambda x:remove_non_ascii(x))\n",
    "cd['clean_ReviewContentPos'] = cd['clean_ReviewContentPos'].apply(lambda x:lemma(x))\n",
    "cd['clean_ReviewContentPos'] = cd['clean_ReviewContentPos'].apply(lambda x:convert_multiple_spaces(x))\n",
    "cd['clean_ReviewContentPos'] = cd['clean_ReviewContentPos'].apply(lambda x:remove_first_end_spaces(x))\n",
    "\n",
    "cd['clean_ReviewContentNeg'] = cd['ReviewContentNeg'].apply(lambda x:remove_punctuation_without_full(x))\n",
    "cd['clean_ReviewContentNeg'] = cd['clean_ReviewContentNeg'].apply(lambda x:remove_numbers(x))\n",
    "cd['clean_ReviewContentNeg'] = cd['clean_ReviewContentNeg'].apply(lambda x:remove_non_ascii(x))\n",
    "cd['clean_ReviewContentNeg'] = cd['clean_ReviewContentNeg'].apply(lambda x:lemma(x))\n",
    "cd['clean_ReviewContentNeg'] = cd['clean_ReviewContentNeg'].apply(lambda x:convert_multiple_spaces(x))\n",
    "cd['clean_ReviewContentNeg'] = cd['clean_ReviewContentNeg'].apply(lambda x:remove_first_end_spaces(x))\n",
    "\n",
    "cd['clean_ReviewContentNeu'] = cd['ReviewContentNeu'].apply(lambda x:remove_punctuation_without_full(x))\n",
    "cd['clean_ReviewContentNeu'] = cd['clean_ReviewContentNeu'].apply(lambda x:remove_numbers(x))\n",
    "cd['clean_ReviewContentNeu'] = cd['clean_ReviewContentNeu'].apply(lambda x:remove_non_ascii(x))\n",
    "cd['clean_ReviewContentNeu'] = cd['clean_ReviewContentNeu'].apply(lambda x:lemma(x))\n",
    "cd['clean_ReviewContentNeu'] = cd['clean_ReviewContentNeu'].apply(lambda x:convert_multiple_spaces(x)) \n",
    "cd['clean_ReviewContentNeu'] = cd['clean_ReviewContentNeu'].apply(lambda x:remove_first_end_spaces(x)) \n",
    "\n",
    "print('Preprocessing Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc03a593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1ae5bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.07 ms (started: 2022-04-29 06:56:30 +00:00)\n"
     ]
    }
   ],
   "source": [
    "raw0 = cd.loc[:,'clean_Reviewcontent'].to_list()\n",
    "raw1 = cd.loc[:,'clean_ReviewContentPos'].to_list()\n",
    "raw2 = cd.loc[:,'clean_ReviewContentNeg'].to_list()\n",
    "raw3 = cd.loc[:,'clean_ReviewContentNeu'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e61fee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 48 ms (started: 2022-04-29 06:56:30 +00:00)\n"
     ]
    }
   ],
   "source": [
    "rem_all = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "def remove_punctuation_without_gap(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in rem_all])\n",
    "    # punctuationfree = punctuationfree.replace(' ','')\n",
    "    return punctuationfree\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_lst = stopwords.words('english')\n",
    "sw = set(['dont','nothing','couldnt','wouldnt','doesnt','mustnt',\n",
    "          'couldn','wouldn','doesn','mustn','neednt','needn','wont',\n",
    "          'unfortunately','havent','arent','haven','aren','cant',\n",
    "          'didnt','didn','shouldnt','shouldn','werent','weren',\n",
    "          'shant','don','cannot','sorry','awfully','not'])\n",
    "for word in stopwords_lst:\n",
    "    sw.add(remove_punctuation_without_gap(word))\n",
    "    \n",
    "import requests\n",
    "stopwords_list = requests.get(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").content\n",
    "stopwords = list(set(stopwords_list.decode().splitlines()))\n",
    "for word in stopwords:\n",
    "    sw.add(remove_punctuation_without_gap(word))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    now = text\n",
    "    now = now.strip().split()\n",
    "    now = [word for word in now if word not in sw and len(word)>2]\n",
    "    text = \"\"\n",
    "    for i in range(len(now)):\n",
    "        text = text + now[i] + \" \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a343f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.82 ms (started: 2022-04-29 06:56:30 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def aspect_based1(sentences):\n",
    "#     print(sentences)\n",
    "    aspects = []\n",
    "    for sentence in sentences:\n",
    "        if sentence == '':\n",
    "            continue\n",
    "        doc = nlp(sentence)\n",
    "        descriptive_term = []\n",
    "        target = []\n",
    "        adv = []\n",
    "        negation = []\n",
    "        descriptive_term_i = []\n",
    "        target_i = []\n",
    "        adv_i = []\n",
    "        negation_i = []\n",
    "        sep = []\n",
    "        sep_i = []\n",
    "        for token in doc:\n",
    "            if token.pos_ == 'PUNCT' or token.pos_ == 'CCONJ':\n",
    "                sep.append(token.text.lower())\n",
    "                sep_i.append(token.i)\n",
    "                continue\n",
    "            if (token.dep_ == 'neg' and token.pos_ =='PART') or token.text.lower()=='no':\n",
    "                negation.append(token.text.lower())\n",
    "                negation_i.append(token.i)\n",
    "                continue\n",
    "            if remove_punctuation_without_gap(token.text.lower()) in sw:\n",
    "                continue\n",
    "            if token.pos_ == 'VERB' or token.pos_ == 'ADV':\n",
    "                adv.append(token.text.lower())\n",
    "                adv_i.append(token.i)\n",
    "            if token.pos_ == 'NOUN':\n",
    "                target.append(token.text.lower())\n",
    "                target_i.append(token.i)\n",
    "            if token.pos_ == 'ADJ':\n",
    "                descriptive_term.append(token.text.lower())\n",
    "                descriptive_term_i.append(token.i)\n",
    "            if token.pos_ == 'ADJ':\n",
    "                prepend = {}\n",
    "                for child in token.children:\n",
    "                    if remove_punctuation_without_gap(child.text.lower()) in sw:\n",
    "                        continue\n",
    "                    if child.pos_ == 'ADJ' or child.pos_ == 'ADV':\n",
    "                        prepend[child.text.lower()] = child.i\n",
    "                        if child.text.lower() in descriptive_term:\n",
    "                            ind = descriptive_term.index(child.text.lower())\n",
    "                            descriptive_term.pop(ind)\n",
    "                            descriptive_term_i.pop(ind)\n",
    "                        if token.text.lower() in descriptive_term:\n",
    "                            ind = descriptive_term.index(token.text.lower())\n",
    "                            descriptive_term.pop(ind)\n",
    "                            descriptive_term_i.pop(ind)\n",
    "                if len(prepend) > 0:\n",
    "                    prepend[token.text.lower()] = token.i\n",
    "                    prepend_sorted = sorted(prepend.items(), key=lambda x: x[1]) \n",
    "                    text_to_add = ''\n",
    "                    temp = []\n",
    "                    for i in range(len(prepend_sorted)-1):\n",
    "                        text_to_add += prepend_sorted[i][0] + \" \"\n",
    "                        temp.append(prepend_sorted[i][1])\n",
    "                    text_to_add += prepend_sorted[len(prepend_sorted)-1][0]\n",
    "                    temp.append(prepend_sorted[len(prepend_sorted)-1][1])\n",
    "                    descriptive_term.append(text_to_add)\n",
    "                    descriptive_term_i.append(temp)\n",
    "        to_del = []\n",
    "        for i in range(len(adv_i)):\n",
    "            now = adv_i[i]\n",
    "            for j in range(len(descriptive_term_i)):\n",
    "                test = descriptive_term_i[j]\n",
    "                if type(test) == list:\n",
    "                    for k in range(len(test)):\n",
    "                        if now == test[k]:\n",
    "                            to_del.append(now)\n",
    "                else:\n",
    "                    if now == test:\n",
    "                        to_del.append(now)\n",
    "        for i in range(len(to_del)):\n",
    "            ind = adv_i.index(to_del[i])\n",
    "            adv.pop(ind)\n",
    "            adv_i.pop(ind)\n",
    "        \n",
    "        for i in range(len(descriptive_term_i)):\n",
    "            test = descriptive_term_i[i]\n",
    "            if type(test) == list:\n",
    "                descriptive_term_i[i] = np.mean(test)\n",
    "                \n",
    "        for i in range(len(negation_i)):\n",
    "            test = negation_i[i]\n",
    "            if type(test) == list:\n",
    "                negation_i[i] = np.mean(test)\n",
    "                \n",
    "        for i in range(len(adv_i)):\n",
    "            test = adv_i[i]\n",
    "            if type(test) == list:\n",
    "                adv_i[i] = np.mean(test)\n",
    "                \n",
    "        for i in range(len(negation_i)):\n",
    "            min_dis = 1000\n",
    "            now = -1\n",
    "            typ = -1\n",
    "            for j in range(len(descriptive_term_i)):\n",
    "                if descriptive_term_i[j] > negation_i[i] and descriptive_term_i[j] - negation_i[i] < min_dis:\n",
    "                    min_dis = descriptive_term_i[j] - negation_i[i]\n",
    "                    now = j\n",
    "                    typ = 1\n",
    "            for j in range(len(adv_i)):\n",
    "                if adv_i[j] > negation_i[i] and adv_i[j] - negation_i[i] < min_dis:\n",
    "                    min_dis = adv_i[j] - negation_i[i]\n",
    "                    now = j\n",
    "                    typ = 2\n",
    "            if now == -1:\n",
    "                for j in range(len(descriptive_term_i)):\n",
    "                    if abs(descriptive_term_i[j] - negation_i[i]) < min_dis:\n",
    "                        min_dis = descriptive_term_i[j] - negation_i[i]\n",
    "                        now = j\n",
    "                        typ = 1\n",
    "                for j in range(len(adv_i)):\n",
    "                    if abs(adv_i[j] - negation_i[i]) < min_dis:\n",
    "                        min_dis = adv_i[j] - negation_i[i]\n",
    "                        now = j\n",
    "                        typ = 2\n",
    "            if typ == 1:\n",
    "                descriptive_term[now] = negation[i] + ' ' + descriptive_term[now]\n",
    "            if typ == 2:\n",
    "                adv[now] = negation[i] + ' ' + adv[now]\n",
    "                    \n",
    "        aspects.append({'sentence':sentence,'aspect': target,\n",
    "                        'description': descriptive_term,'adv':adv,\n",
    "                        'negation':negation,'aspect_i': target_i,\n",
    "                        'description_i': descriptive_term_i,'adv_i':adv_i,\n",
    "                        'negation_i':negation_i,'sep':sep,'sep_i':sep_i})\n",
    "#     print_list(aspects)\n",
    "    return aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "009e308f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2 ms (started: 2022-04-29 06:56:30 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def combine_dictionary(a,b):\n",
    "    final = {}\n",
    "    if len(a) == 0:\n",
    "        return b\n",
    "    elif len(b) == 0:\n",
    "        return a\n",
    "    for itm in a:\n",
    "        final[itm] = [a[itm]]\n",
    "    for itm in b:\n",
    "        if itm in final:\n",
    "            now = final[itm]\n",
    "            now.append(b[itm])\n",
    "        else:\n",
    "            final[itm] = [b[itm]]\n",
    "    for itm in final:\n",
    "        now = final[itm]\n",
    "        score = np.mean(now)\n",
    "        final[itm] = score\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28fd26de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.92 ms (started: 2022-04-29 06:56:30 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def aspect_relation1(dict_now):\n",
    "    ans = {}\n",
    "    return ans\n",
    "\n",
    "def aspect_relation2(dict_now,column):\n",
    "    ans = {}\n",
    "    target = dict_now['aspect']\n",
    "    sen = dict_now['sentence']\n",
    "    if column == 'POSITIVE':\n",
    "        for i in range(len(target)):\n",
    "            ans[target[i]] = 1\n",
    "    elif column == 'NEGATIVE':\n",
    "        for i in range(len(target)):\n",
    "            ans[target[i]] = -1\n",
    "    else:\n",
    "        s1 = bert_distilbert(sen)[0]\n",
    "        r1 = 1000\n",
    "        if s1['label'] == 'POSITIVE':\n",
    "            r1 = s1['score']\n",
    "        elif s1['label'] == 'NEGATIVE':\n",
    "            r1 = -s1['score']\n",
    "            \n",
    "        s2 = roberta_english(sen)[0]\n",
    "        r2 = 1000\n",
    "        if s2['label'] == 'POSITIVE':\n",
    "            r2 = s2['score']\n",
    "        elif s2['label'] == 'NEGATIVE':\n",
    "            r2 = -s2['score']\n",
    "            \n",
    "        score = round((r1+r2)/2,5)\n",
    "        for i in range(len(target)):\n",
    "            ans[target[i]] = score\n",
    "            #print(ans)\n",
    "    return ans\n",
    "\n",
    "def aspect_relation3(dict_now):\n",
    "    ans = {}\n",
    "    target = dict_now['aspect']\n",
    "    sen = dict_now['sentence']\n",
    "\n",
    "    s1 = bert_distilbert(sen)[0]\n",
    "    r1 = 1000\n",
    "    if s1['label'] == 'POSITIVE':\n",
    "        r1 = s1['score']\n",
    "    elif s1['label'] == 'NEGATIVE':\n",
    "        r1 = -s1['score']\n",
    "\n",
    "    s2 = roberta_english(sen)[0]\n",
    "    r2 = 1000\n",
    "    if s2['label'] == 'POSITIVE':\n",
    "        r2 = s2['score']\n",
    "    elif s2['label'] == 'NEGATIVE':\n",
    "        r2 = -s2['score']\n",
    "\n",
    "    score = round((r1+r2)/2,5)\n",
    "    ans[target[0]] = score\n",
    "    #print(ans)\n",
    "    return ans\n",
    "\n",
    "def aspect_relation4(dict_now):\n",
    "    ans = {}\n",
    "    target = dict_now['aspect']\n",
    "    sen = dict_now['sentence']\n",
    "\n",
    "    s1 = bert_distilbert(sen)[0]\n",
    "    r1 = 1000\n",
    "    if s1['label'] == 'POSITIVE':\n",
    "        r1 = s1['score']\n",
    "    elif s1['label'] == 'NEGATIVE':\n",
    "        r1 = -s1['score']\n",
    "\n",
    "    s2 = roberta_english(sen)[0]\n",
    "    r2 = 1000\n",
    "    if s2['label'] == 'POSITIVE':\n",
    "        r2 = s2['score']\n",
    "    elif s2['label'] == 'NEGATIVE':\n",
    "        r2 = -s2['score']\n",
    "\n",
    "    score = round((r1+r2)/2,5)\n",
    "    for i in range(len(target)):\n",
    "        ans[target[i]] = score\n",
    "        #print(ans)\n",
    "    return ans\n",
    "\n",
    "def aspect_relation5(dict_now):\n",
    "    ans = {}\n",
    "    target = dict_now['aspect']\n",
    "    sen = dict_now['sentence']\n",
    "\n",
    "    s1 = bert_distilbert(sen)[0]\n",
    "    r1 = 1000\n",
    "    if s1['label'] == 'POSITIVE':\n",
    "        r1 = s1['score']\n",
    "    elif s1['label'] == 'NEGATIVE':\n",
    "        r1 = -s1['score']\n",
    "\n",
    "    s2 = roberta_english(sen)[0]\n",
    "    r2 = 1000\n",
    "    if s2['label'] == 'POSITIVE':\n",
    "        r2 = s2['score']\n",
    "    elif s2['label'] == 'NEGATIVE':\n",
    "        r2 = -s2['score']\n",
    "\n",
    "    score = round((r1+r2)/2,5)\n",
    "    ans[target[0]] = score\n",
    "    #print(ans)\n",
    "    return ans\n",
    "\n",
    "def aspect_relation6(dict_now):\n",
    "    ans = {}\n",
    "    target = dict_now['aspect']\n",
    "    target_i = dict_now['aspect_i']\n",
    "    words1 = dict_now['description']\n",
    "    words2 = dict_now['adv']\n",
    "    wordsi_1 = dict_now['description_i']\n",
    "    wordsi_2 = dict_now['adv_i']\n",
    "    wordsf_1 = []\n",
    "    wordsf_2 = []\n",
    "    for i in range(len(wordsi_1)):\n",
    "        now = wordsi_1[i]\n",
    "        if type(now) == list:\n",
    "            wordsf_1.append(np.mean(now))\n",
    "        else:\n",
    "            wordsf_1.append(now)\n",
    "            \n",
    "    for i in range(len(wordsi_2)):\n",
    "        now = wordsi_2[i]\n",
    "        if type(now) == list:\n",
    "            wordsf_2.append(np.mean(now))\n",
    "        else:\n",
    "            wordsf_2.append(now)\n",
    "    X = target\n",
    "    XX = target_i\n",
    "    Y = words1 + words2\n",
    "    YY = wordsf_1 + wordsf_2\n",
    "    \n",
    "    y = copy.deepcopy(Y)\n",
    "    yy = copy.deepcopy(YY)\n",
    "    ans_list = []\n",
    "    ind_list = []\n",
    "    for i in range(len(XX)):\n",
    "        min_dis = 1000\n",
    "        now = 0\n",
    "        for j in range(len(YY)):\n",
    "            if abs(YY[j]-XX[i]) < min_dis:\n",
    "                min_dis = abs(YY[j]-XX[i])\n",
    "                now = j\n",
    "        ans_list.append(Y[now])\n",
    "        ind_list.append(now)\n",
    "    ind_list = np.sort(list(set(ind_list)))[::-1].tolist()\n",
    "    \n",
    "    for i in range(len(ind_list)):\n",
    "        index = ind_list[i]\n",
    "        y.pop(index)\n",
    "        yy.pop(index)\n",
    "    \n",
    "    relation = []\n",
    "    for i in range(len(ans_list)):\n",
    "        relation.append([target[i],ans_list[i]])\n",
    "    \n",
    "    for i in range(len(yy)):\n",
    "        min_dis = 1000\n",
    "        now = 0\n",
    "        for j in range(len(XX)):\n",
    "            if abs(yy[i]-XX[j]) < min_dis:\n",
    "                min_dis = abs(yy[i]-XX[j])\n",
    "                now = j\n",
    "        relation.append([X[now],y[i]])\n",
    "        \n",
    "    \n",
    "    for i in range(len(target)):\n",
    "        ans[target[i]] = []\n",
    "    for i in range(len(relation)):\n",
    "        word = relation[i][0]\n",
    "        sen = relation[i][1]\n",
    "        \n",
    "        s1 = bert_distilbert(sen)[0]\n",
    "        r1 = 1000\n",
    "        if s1['label'] == 'POSITIVE':\n",
    "            r1 = s1['score']\n",
    "        elif s1['label'] == 'NEGATIVE':\n",
    "            r1 = -s1['score']\n",
    "\n",
    "        s2 = roberta_english(sen)[0]\n",
    "        r2 = 1000\n",
    "        if s2['label'] == 'POSITIVE':\n",
    "            r2 = s2['score']\n",
    "        elif s2['label'] == 'NEGATIVE':\n",
    "            r2 = -s2['score']\n",
    "\n",
    "        score = round((r1+r2)/2,5)\n",
    "        \n",
    "        list_now = ans[word]\n",
    "        list_now.append(score)\n",
    "        ans[word] = list_now\n",
    "    \n",
    "    for itm in ans:\n",
    "        value = ans[itm]\n",
    "        value_mean = np.mean(value)\n",
    "        ans[itm] = value_mean\n",
    "       \n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64dd2cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.17 ms (started: 2022-04-29 06:56:30 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def type_detection(ab,column):\n",
    "    rows = len(ab)\n",
    "    final = {}\n",
    "    for i in range(rows):\n",
    "        dict_now = ab[i]\n",
    "        NC = len(dict_now['aspect'])\n",
    "        DC = len(dict_now['description']) + len(dict_now['adv'])\n",
    "        if NC == 0:\n",
    "#             print('Type 1')\n",
    "            ans = aspect_relation1(dict_now)\n",
    "            final = combine_dictionary(ans,final)\n",
    "        elif DC == 0:\n",
    "#             print('Type 2')\n",
    "            ans = aspect_relation2(dict_now,column)\n",
    "#             Print(column)\n",
    "            final = combine_dictionary(ans,final)\n",
    "        elif NC == 1 and DC == 1:\n",
    "#             print('Type 3')\n",
    "            ans = aspect_relation3(dict_now)\n",
    "            final = combine_dictionary(ans,final)\n",
    "        elif NC > 1 and DC == 1:\n",
    "#             print('Type 4')\n",
    "            ans = aspect_relation4(dict_now)\n",
    "            final = combine_dictionary(ans,final)\n",
    "        elif NC == 1 and DC > 1:\n",
    "#             print('Type 5')\n",
    "            ans = aspect_relation5(dict_now)\n",
    "            final = combine_dictionary(ans,final)\n",
    "        elif NC > 1 and DC > 1:\n",
    "#             print('Type 6')\n",
    "            ans = aspect_relation6(dict_now)\n",
    "            final = combine_dictionary(ans,final)\n",
    "        else:\n",
    "            print('Type X')\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b04e8736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1345, 2)\n",
      "time: 39.2 ms (started: 2022-04-29 06:56:30 +00:00)\n"
     ]
    }
   ],
   "source": [
    "helper = pd.read_csv('aspect_dictionary.csv')\n",
    "#helper=helper[(helper['Class']!='Z')&(helper['Class']!='z')]\n",
    "print(helper.shape)\n",
    "helper['Phrase'] = helper['Phrase'].astype(str)\n",
    "helper['Class'] = helper['Class'].astype(str)\n",
    "keywords = list(set(helper.loc[:,'Class'].to_list()))\n",
    "classes_dict = {}\n",
    "for i in range(len(helper)):\n",
    "    add = lemma(remove_first_end_spaces(helper.loc[i,'Phrase']))\n",
    "    #print(add)\n",
    "    add = remove_punctuation_without_full(add)\n",
    "    add = remove_first_end_spaces(add)\n",
    "    classes_dict[add] = helper.loc[i,'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8183e28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b0a8e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 27.7 s (started: 2022-04-29 06:56:30 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from scipy import spatial\n",
    "glove_model = api.load('glove-twitter-25')\n",
    "\n",
    "def find_aspect(aspect,keywords):\n",
    "    sim = []\n",
    "    sample_glove_embedding1=glove_model[aspect.lower()]\n",
    "    for i in range(len(keywords)):\n",
    "        sample_glove_embedding2=glove_model[keywords[i].lower()]\n",
    "        res = 1 - spatial.distance.cosine(sample_glove_embedding1, sample_glove_embedding2)\n",
    "        sim.append(res)\n",
    "    x = np.argmax(sim)\n",
    "    return keywords[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d64f9c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 611 µs (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def find_class(aspect, keywords, classes_dict):\n",
    "    aspect = lemma(remove_first_end_spaces(aspect))\n",
    "    aspect = remove_punctuation_without_full(aspect)\n",
    "    aspect = remove_first_end_spaces(aspect)\n",
    "    if aspect in classes_dict:\n",
    "        if classes_dict[aspect] == 'dump':\n",
    "            ret = -1\n",
    "            return ret\n",
    "        return classes_dict[aspect]\n",
    "    try:\n",
    "        ret = find_aspect(aspect, keywords)\n",
    "    except:\n",
    "        ret = -1\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a30d6373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 46.4 ms (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def collate_classes(dict_now,keywords,classes_dict):\n",
    "    ans = {}\n",
    "    for itm in dict_now:\n",
    "        class_add = find_class(itm,keywords,classes_dict)\n",
    "        if class_add == -1:\n",
    "            continue\n",
    "        if class_add in ans:\n",
    "            now = ans[class_add]\n",
    "            now.append(dict_now[itm])\n",
    "            ans[class_add] = now\n",
    "        else:\n",
    "            ans[class_add] = [dict_now[itm]]\n",
    "    for itm in ans:\n",
    "        now = ans[itm]\n",
    "        score = np.mean(now)\n",
    "        ans[itm] = round(score,5)\n",
    "        #print(now)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb662d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 13 ms (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def ensemble_sentiment(text):\n",
    "    snip = text\n",
    "    now = bert_distilbert(snip)[0]\n",
    "    x = 0\n",
    "    y = 0\n",
    "    if now['label'] == 'POSITIVE':\n",
    "        x += now['score']\n",
    "    else:\n",
    "        x -= now['score']\n",
    "    temp = roberta_english(snip)[0]\n",
    "    if now['label'] == 'POSITIVE':\n",
    "        y += now['score']\n",
    "    else:\n",
    "        y -= now['score']\n",
    "#     print(\"Overall Sentiment Score:\",round((x+y)/2,5))\n",
    "    return round((x+y)/2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a37ec69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.11 ms (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def find_aspect_class_relation(relation, td, keywords, classes_dict):\n",
    "    for itm in td:\n",
    "        relation[itm] = find_class(itm, keywords, classes_dict)\n",
    "    return relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b0fb28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9af86a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "813a29d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Must have equal len keys and value when setting with an iterable\n",
      "time: 200 ms (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "# for i in range(cd.shape[0]):\n",
    "for i in range(cd.shape[0]):\n",
    "#     print('Run:',i)\n",
    "    try:\n",
    "        text = cd.loc[i,'clean_Reviewcontent']\n",
    "        text_lst=[]\n",
    "        if len(text)>1800:\n",
    "            res_first, res_second = text[:len(text)//2],text[len(text)//2:]\n",
    "            text_lst.append(res_first)\n",
    "            text_lst.append(res_second)\n",
    "        else:\n",
    "            text_lst.append(text)\n",
    "        score_lst=[]\n",
    "        for text1 in text_lst:\n",
    "            a_score = ensemble_sentiment(text1)\n",
    "            score_lst.append(a_score)\n",
    "        cd.loc[i,'Sentiment Score'] =sum(score_lst)/len(score_lst)\n",
    "#         cd.loc[i,'Sentiment Score'] = ensemble_sentiment(text)\n",
    "        pol_prop = {}\n",
    "        index = i\n",
    "        snip = raw1[index]\n",
    "        text = snip\n",
    "        text = snip.split('.')\n",
    "        ab = aspect_based1(text)\n",
    "        # pos_display(snip)\n",
    "        # dataframe_display(snip)\n",
    "        # ensemble_sentiment(snip)\n",
    "        # polarity_ab(ab)\n",
    "        \n",
    "        relation = {}\n",
    "\n",
    "        td = type_detection(ab,'POSITIVE')\n",
    "        relation = find_aspect_class_relation(relation, td, keywords, classes_dict)\n",
    "        c1 = collate_classes(td,keywords,classes_dict)\n",
    "        #print(keywords)\n",
    "\n",
    "        index = i\n",
    "        snip = raw2[index]\n",
    "        text = snip\n",
    "        text = snip.split('.')\n",
    "        ab = aspect_based1(text)\n",
    "        # pos_display(snip)\n",
    "        # dataframe_display(snip)\n",
    "        # ensemble_sentiment(snip)\n",
    "        # polarity_ab(ab)\n",
    "\n",
    "        td = type_detection(ab,'NEGATIVE')\n",
    "        relation = find_aspect_class_relation(relation, td, keywords, classes_dict)\n",
    "        c2 = collate_classes(td,keywords,classes_dict)\n",
    "        #print(td)\n",
    "\n",
    "        index = i\n",
    "        snip = raw3[index]\n",
    "        text = snip\n",
    "        text = snip.split('.')\n",
    "        ab = aspect_based1(text)\n",
    "        # pos_display(snip)\n",
    "        # dataframe_display(snip)\n",
    "        # ensemble_sentiment(snip)\n",
    "        # polarity_ab(ab)\n",
    "\n",
    "        td = type_detection(ab,'NEUTRAL')\n",
    "        relation = find_aspect_class_relation(relation, td, keywords, classes_dict)\n",
    "        c3 = collate_classes(td,keywords,classes_dict)\n",
    "        pol_prop = combine_dictionary(pol_prop,c1)\n",
    "        pol_prop = combine_dictionary(pol_prop,c2)\n",
    "        pol_prop = combine_dictionary(pol_prop,c3)\n",
    "\n",
    "#         for itm in relation:\n",
    "#             if itm in classes_dict:\n",
    "#                 continue\n",
    "#             print(itm,'--->',relation[itm])\n",
    "   \n",
    "        cd.loc[i,'Relation'] = str(relation)\n",
    "        cd.loc[i,'Properties'] = str(pol_prop)\n",
    "        if len(pol_prop)>0:\n",
    "            for j in range (0,len(pol_prop)):\n",
    "                dictt = pol_prop\n",
    "                if list(pol_prop.values())[j] < 0.0:\n",
    "                    dictt.update({list(pol_prop.keys())[j]:[list(pol_prop.values())[j],'Negative']})\n",
    "                elif list(pol_prop.values())[j] > 0.0:\n",
    "                    dictt.update({list(pol_prop.keys())[j]:[list(pol_prop.values())[j],'Positive']})\n",
    "                else:\n",
    "                    dictt.update({list(pol_prop.keys())[j]:[list(pol_prop.values())[j],'Neutral']})\n",
    "\n",
    "\n",
    "            cd.loc[i,'Strength'] = str(dictt)\n",
    "        else:\n",
    "            cd.loc[i,'Strength'] = {}\n",
    "        print(str(dictt))\n",
    "        exit()\n",
    "        \n",
    "#         print('---------------------------------')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(i,e)\n",
    "        continue\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(exc_type, fname, exc_tb.tb_lineno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "398d19b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.6 ms (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "cd.loc[cd['Sentiment_Score']==0.0,'Sentiment_Score']=cd['Sentiment Score']\n",
    "\n",
    "cd=cd[['hotelcode', 'review_header', 'review_content', 'reviewid_new', 'id',\n",
    "       'review_score', 'row', 'Reviewcontent', 'ReviewContentPos',\n",
    "       'ReviewContentNeg', 'ReviewContentNeu', 'Sentiment_Score',\n",
    "       'clean_Reviewcontent', 'clean_ReviewContentPos',\n",
    "       'clean_ReviewContentNeg', 'clean_ReviewContentNeu',\n",
    "       'Relation', 'Properties', 'Strength']]\n",
    "\n",
    "cd['Sentiment_Score']=cd['Sentiment_Score'].round(2)\n",
    "\n",
    "cd['Sentiment_Score']=cd['Sentiment_Score'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97e9b7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hotelcode', 'review_header', 'review_content', 'reviewid_new', 'id',\n",
       "       'review_score', 'row', 'Reviewcontent', 'ReviewContentPos',\n",
       "       'ReviewContentNeg', 'ReviewContentNeu', 'Sentiment_Score',\n",
       "       'clean_Reviewcontent', 'clean_ReviewContentPos',\n",
       "       'clean_ReviewContentNeg', 'clean_ReviewContentNeu', 'Relation',\n",
       "       'Properties', 'Strength'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.49 ms (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "cd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fab7526b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.43 ms (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "cd.loc[cd['Sentiment_Score'].isnull(),'Sentiment_Score']=0.0\n",
    "cd.loc[cd['Sentiment_Score']< 0,'polarity']=\"negative\"\n",
    "cd.loc[(cd['Sentiment_Score']> 0.10),'polarity']=\"positive\"\n",
    "cd.loc[(cd['Sentiment_Score']<=0.10) & (cd['Sentiment_Score']>= 0),'polarity']=\"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29045976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.54 ms (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# cd['Properties'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7280f980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.04 ms (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# cd[(cd['Properties']=='{}')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8bb206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64a1a268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 58.5 ms (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "cd.to_excel('Todays_sample_with_Aayush_code6.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3c9f29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 240 µs (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6b084d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.31 ms (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# cd['Properties'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05bfda91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.27 ms (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# {'fnb & bar': 0.99856}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28e4abee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 978 µs (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# pol_prop={'fnb & bar': 0.99856}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1952ee95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 912 µs (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# list(pol_prop.values())[0]<0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "272c3cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 880 µs (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# dictt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "893bcb9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 610 µs (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# dictt = pol_prop\n",
    "\n",
    "# if [list(pol_prop.values())[0]<0]:\n",
    "#     str_prop=dictt.update({list(pol_prop.keys())[0]:[list(pol_prop.values())[0],'Negative']})\n",
    "#     print(str_prop)\n",
    "# elif [list(pol_prop.values())[0]>0]:\n",
    "#     str_prop=dictt.update({list(pol_prop.keys())[0]:[list(pol_prop.values())[0],'Positive']})\n",
    "#     print(str_prop)\n",
    "# else:\n",
    "#     str_prop=dictt.update({list(pol_prop.keys())[0]:[list(pol_prop.values())[0],'Neutral']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e277583b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4732dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 617 µs (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# list(pol_prop.values())[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5efd8d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.26 ms (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# [list(pol_prop.values())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e708299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.08 ms (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# dict = pol_prop\n",
    "\n",
    "# if [list(pol_prop.values())[0]<1]:\n",
    "#     str_prop=dict.update({list(pol_prop.keys())[0]:[list(pol_prop.values())[0],'Negative']})\n",
    "# elif [list(pol_prop.values())[0]>1]:\n",
    "#     str_prop=dict.update({list(pol_prop.keys())[0]:[list(pol_prop.values())[0],'Positive']})\n",
    "# else:\n",
    "#     str_prop=dict.update({list(pol_prop.keys())[0]:[list(pol_prop.values())[0],'Neutral']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "05b635d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 743 µs (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# pol_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8248d70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 963 µs (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# list(pol_prop.values())[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1266d75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 504 µs (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# cd['Properties']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6725f6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 373 µs (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# helper[helper['Class']=='z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "623b0268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 439 µs (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# list(pol_prop.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e800315e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.43 ms (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# cd['Properties']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad76a233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 533 µs (started: 2022-04-29 06:56:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# dict\n",
    "\n",
    "# dict\n",
    "\n",
    "# cd['Properties']\n",
    "\n",
    "# cd[cd['Reviewcontent'].str.contains('There are no comments',case=False)]\n",
    "\n",
    "# cd[cd['Reviewcontent'].str.contains('There are no comments',case=False)]\n",
    "\n",
    "# cd[cd['Reviewcontent']=='']\n",
    "\n",
    "# cd\n",
    "\n",
    "# cd['Properties'][0]\n",
    "\n",
    "# cd.loc[(cd['Reviewcontent'].str.contains(\"positive\",case=False)) & (cd['Reviewcontent'].str.contains(\"negative\",case=False)),'Sentiment Score']=cd['Sentiment_Score']\n",
    "\n",
    "# aa=cd[['hotelcode', 'review_header', 'review_content', 'reviewid_new', 'id',\n",
    "#        'review_score','Sentiment Score','Properties']]\n",
    "\n",
    "# aa.to_excel('sample_aayush_code.xlsx',index=False)\n",
    "\n",
    "\n",
    "\n",
    "# a = list(np.arange(start=-1, stop=1, step=0.05))\n",
    "# OldMax = max(a)\n",
    "# OldMin = min(a)\n",
    "\n",
    "# NewMax = 5\n",
    "# NewMin = 0\n",
    "\n",
    "# OldRange = (OldMax - OldMin)  \n",
    "# NewRange = (NewMax - NewMin)  \n",
    "\n",
    "# ab['Sentiment Score'] = (((ab['Sentiment Score'] - OldMin) * NewRange) / OldRange) + NewMin\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "# ab\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# cd.to_excel('sample_results_testing.xlsx',index=False)\n",
    "\n",
    "# cd[cd['Sentiment Score']==0.0]\n",
    "\n",
    "# cd[cd['Reviewcontent'].str.contains('[Positive]')]\n",
    "\n",
    "# cd[cd['ReviewContentPos']=='it’s clean']\n",
    "\n",
    "# cd[cd['ReviewContentPos'].notnull()]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
